<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Meghna</title>
  
  <meta name="author" content="Meghna">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Meghna</name>
              </p>
              <p>I am a Ph.D. Research Scholar under the supervision of <a target="_blank" href="https://sites.google.com/view/badrisubudhi/home?pli=1">Dr. Badri N Subudhi</a> and <a target="_blank" href="https://www.iitjammu.ac.in/faculty/~ankurbansal">Dr. Ankur Bansal</a> in the EE Department at <a target="_blank" href="https://www.iitjammu.ac.in/">Indian Institute of Technology Jammu, India</a> since 2021. I primarily work in the area of image processing, computer vision, graph learning, and underwater surveillance. 
              </p>
              <p>
                I have completed my M.E. in Electronics and Communication from <a target="_blank" href="https://https://www.iisc.ac.in/">Thapar Institute of Engineering and Technology</a>Patiala, India.



              </p>
              <p style="text-align:center">
                <a target="_blank" href="mailto:meghna@iitjammu.ac.in">Email</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?user=q9jVEqkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://www.linkedin.com/in/meghna-kapoor-166b15147/">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/dd.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dd.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           
	 <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
		 <p> 17/08/2024- One paper has been accepted in ICPR 2024.</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
		<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
		      I am currently engaged in the field of underwater surveillance, focusing on mitigating degradations in complex underwater environments. My work involves implementing various methodologies to enhance image clarity, preserve details, and accurately detect objects in challenging underwater conditions.
	      </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

		
		<!------------------------------Paper 3--------------------------------->	
			
         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img style="width: 160px;" src='images/3.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Project and Pool: An Action Localization network for localizing actions in Untrimmed Videos</papertitle>
              </a>
              <br>
         <a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/post/phd-students-electrical">Himanshu Singh</a>, 
        <a target="_blank" href="https://www.linkedin.com/in/avijit-dey-188a351a8/?originalSubdomain=in">, Avijit Dey </a>, 
	<a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/faculty-list/~badrinsubudhi">Badri N Subudhi</a>,
	<a target="_blank" href="https://iitjammu.ac.in/computer_science_engineering/faculty-list/~vinitjakhetiya">Vinit Jakhetiya</a>,
              <em></sup>in Proceedings of the International Conference on Pattern Recognition (ICPR), Kolkata, India</em>, 2024.
              <br>
              <p></p>
              <p>
               In this paper, the proposed algorithm follows a three-stage framework for action recognition and localization. In the first stage, spatial and temporal features are projected using a two-layer LSTM module called LSTMProjector, which captures both local and global dependencies in the input video sequences. In the second stage, a latent space projection is performed using a one-dimensional convolutional layer to harmonize the dimensions of spatial and temporal features. In the final stage, a parameter-free temporal pooling block selectively extracts critical information from local clip embeddings, improving the model‚Äôs efficiency in action localization.</p>
            </td>
          </tr>
          
		<!------------------------------Paper 2--------------------------------->
		
         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img style="width: 160px;" src='images/1.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9954149">
                <papertitle>Action Recognition in Dark Videos using Spatio-temporal Features and Bidirectional Encoder Representations from Transformers</papertitle>
              </a>
              <br>
         <a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/post/phd-students-electrical">Himanshu Singh</a>, 
        <a target="_blank" href="https://in.linkedin.com/in/saurabh-suman-a83734199">Saurabh Suman</a>, 
	<a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/faculty-list/~badrinsubudhi">Badri N Subudhi</a>,
	<a target="_blank" href="https://iitjammu.ac.in/computer_science_engineering/faculty-list/~vinitjakhetiya">Vinit Jakhetiya</a>,
        <a target="_blank" href="https://www.isical.ac.in/~ash/">Ashish Ghosh</a>,
              <em></sup>in IEEE Transactions on Artificial Intelligence</em>, 2022.
              <br>
              <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9954149">Paper Link</a>
              <p></p>
              <p>
               In this paper, The proposed algorithm follows two-stages for action recognition. In the first stage, the low-light videos are enhanced using Zero-Reference Deep Curve Estimation (Zero-DCE), followed by the min-max sampling algorithm. In the latter stage, we propose an action classification network to recognize the actions in the enhanced videos.</p>
            </td>
          </tr>
          <!---------------------------Paper 1------------------------------------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img style="width: 160px;" src='images/2.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/21662">
                <papertitle>C3D and Localization Model for Locating and Recognizing the Actions from Untrimmed Videos (Student Abstract)</papertitle>
              </a>
              <br>
        <a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/post/phd-students-electrical">Himanshu Singh</a>, 
        <a target="_blank" href="https://in.linkedin.com/in/tirupati-pallewad-047b69158">Tirupati Pallewad</a>, 
	<a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/faculty-list/~badrinsubudhi">Badri N Subudhi</a>,
	<a target="_blank" href="https://iitjammu.ac.in/computer_science_engineering/faculty-list/~vinitjakhetiya">Vinit Jakhetiya</a>

        
              <em>in Proceedings of the AAAI Conference on Artificial Intelligence, Vancouver Canada</em>, 2021
              <br>
              <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/21662">Paper Link</a>
              <p></p>
	      <p>
		In this paper,  we proposed a technique for action localization and recognition from long untrimmed videos. It consists of C3D CNN model followed by the action mining using the localization model, where the KNN classifier is used. We segment the video into expressible sub-action known as action-bytes. The pseudo labels have been used to train the localization model, which makes the trimmed videos untrimmed for action-bytes. We present experimental results on the recent benchmark trimmed video dataset ‚ÄúThumos14‚Äù.</p>
	</td>
        </tr>



          </tr>				
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p style="text-align:right;font-size:small;">
                Thanks for Visiting my Page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p style="text-align:right;font-size:small;">
                <!-- hitwebcounter Code START -->
<a href="https://www.hitwebcounter.com" target="_blank">
<img src="https://hitwebcounter.com/counter/counter.php?page=8120766&style=0025&nbdigits=5&type=ip&initCount=50" title="Free Counter" Alt="web counter"   border="0" /></a>   
              </p>
            </td>
          </tr>
        </tbody></table>
</body>

</html>
